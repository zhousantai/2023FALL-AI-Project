{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff1a7b4-c282-4b2f-a205-e625614ebbb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T20:42:06.133433Z",
     "iopub.status.busy": "2023-12-25T20:42:06.132867Z",
     "iopub.status.idle": "2023-12-25T20:42:07.972631Z",
     "shell.execute_reply": "2023-12-25T20:42:07.971576Z",
     "shell.execute_reply.started": "2023-12-25T20:42:06.133392Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "749a4424-33a8-4d16-8a87-a70fa41eacfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T20:42:07.974328Z",
     "iopub.status.busy": "2023-12-25T20:42:07.974097Z",
     "iopub.status.idle": "2023-12-25T21:02:29.699680Z",
     "shell.execute_reply": "2023-12-25T21:02:29.697997Z",
     "shell.execute_reply.started": "2023-12-25T20:42:07.974309Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "train_data = pd.read_feather('./input/train.feather')\n",
    "labels = pd.read_csv('/openbayes/input/input0/train_labels.csv')\n",
    "train_data = pd.merge(train_data, labels, on='customer_ID', how='left')  # 合并标签列到训练数据集中\n",
    "test_data = pd.read_feather('./input/test.feather')\n",
    "\n",
    "# 标签\n",
    "label_name = 'target'\n",
    "\n",
    "# 标准化数值特征\n",
    "numerical_cols = [col for col in train_data.columns if col not in ['customer_ID', 'S_2', label_name] and train_data[col].dtype != 'O']\n",
    "\n",
    "# 使用均值填充数值特征的缺失值\n",
    "train_data[numerical_cols] = train_data[numerical_cols].fillna(train_data[numerical_cols].mean())\n",
    "test_data[numerical_cols] = test_data[numerical_cols].fillna(train_data[numerical_cols].mean())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data[numerical_cols] = scaler.fit_transform(train_data[numerical_cols])\n",
    "test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])\n",
    "\n",
    "# 类别特征处理 - 使用 Embedding 层替代 DNN 处理\n",
    "cat_features = [\"B_30\", \"B_38\", \"D_114\", \"D_116\", \"D_117\", \"D_120\", \"D_126\", \"D_63\", \"D_64\", \"D_66\", \"D_68\"]\n",
    "\n",
    "class CategoricalEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super(CategoricalEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "# 处理缺失值和类别特征编码\n",
    "for col in cat_features:\n",
    "    # 使用-1填充缺失值\n",
    "    train_data[col] = train_data[col].astype('category').cat.codes.replace(np.nan,-1)\n",
    "    test_data[col] = test_data[col].astype('category').cat.codes.replace(np.nan,-1)\n",
    "\n",
    "    # 使用训练集中的编码\n",
    "    unique_values_train = set(train_data[col].dropna().unique())\n",
    "    unique_values_test = set(test_data[col].dropna().unique())\n",
    "\n",
    "    # 新增的类别值\n",
    "    new_values = unique_values_test - unique_values_train\n",
    "\n",
    "    # 如果测试集中有新的类别值，将它们映射为一个特殊的编码\n",
    "    if new_values:\n",
    "        mapping = {val: len(unique_values_train) for val in new_values}\n",
    "        test_data[col] = test_data[col].apply(lambda x: mapping.get(x, x))\n",
    "\n",
    "    embedding_dim = min(50, len(unique_values_train) + 1)  # +1 for the padding index\n",
    "    embedding_layer = CategoricalEmbedding(len(unique_values_train) + 1, embedding_dim)\n",
    "\n",
    "    # Apply the embedding to the input data\n",
    "    embedded_values = embedding_layer(torch.LongTensor(train_data[col].values))\n",
    "    # Concatenate the embedded values to the dataframe\n",
    "    train_data = pd.concat([train_data, pd.DataFrame(embedded_values.detach().numpy(), columns=[f'{col}_{i}' for i in range(embedding_dim)])], axis=1)\n",
    "\n",
    "    # Transform on test data\n",
    "    embedded_test_values = embedding_layer(torch.LongTensor(test_data[col].values))\n",
    "    test_data = pd.concat([test_data, pd.DataFrame(embedded_test_values.detach().numpy(), columns=[f'{col}_{i}' for i in range(embedding_dim)])], axis=1)\n",
    "\n",
    "# 删除原始的类别特征列\n",
    "train_data = train_data.drop(cat_features, axis=1)\n",
    "test_data = test_data.drop(cat_features, axis=1)\n",
    "\n",
    "# 时间特征处理\n",
    "time_features = [\"S_2\"]\n",
    "for col in time_features:\n",
    "    train_data[col] = pd.to_datetime(train_data[col])\n",
    "    test_data[col] = pd.to_datetime(test_data[col])\n",
    "    \n",
    "\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=None)\n",
    "\n",
    "\n",
    "# 构建 RNN 数据\n",
    "def prepare_rnn_data(data):\n",
    "    data['customer_ID'] = data['customer_ID']\n",
    "    data[time_features] = data[time_features].apply(lambda x: x.astype(np.int64) // 10**9)\n",
    "\n",
    "    if label_name not in data.columns:\n",
    "        data[label_name] = 0\n",
    "\n",
    "    # 取平均值，大于0.5定为1，反之定为0\n",
    "    data[label_name] = (data.groupby('customer_ID')[label_name].transform('mean') > 0.5).astype(np.float32)\n",
    "\n",
    "    rnn_data = data.groupby('customer_ID').apply(lambda x: x[time_features + [label_name]].values.tolist()).tolist()\n",
    "    rnn_data = [torch.tensor(seq, dtype=torch.float32) for seq in rnn_data]\n",
    "\n",
    "    rnn_data_padded = pad_sequence(rnn_data, batch_first=True, padding_value=0)\n",
    "\n",
    "    rnn_labels = data.groupby('customer_ID')[label_name].mean().values\n",
    "    rnn_labels = (rnn_labels > 0.5).astype(np.float32)\n",
    "    return rnn_data_padded, rnn_labels\n",
    "\n",
    "train_rnn_data, train_rnn_labels = prepare_rnn_data(train_data)\n",
    "val_rnn_data, val_rnn_labels = prepare_rnn_data(val_data)\n",
    "test_rnn_data, _ = prepare_rnn_data(test_data)\n",
    "\n",
    "# 构建 DNN 数据\n",
    "def prepare_dnn_data(data):\n",
    "    data[time_features] = data[time_features].apply(lambda x: x.astype(np.int64) // 10**9)\n",
    "    \n",
    "    dnn_data = data.drop(time_features + [label_name], axis=1)\n",
    "    dnn_data = dnn_data.groupby('customer_ID').mean().reset_index(drop=False)\n",
    "    dnn_data = dnn_data.drop_duplicates('customer_ID')\n",
    "    dnn_data = dnn_data.drop(['customer_ID'], axis=1)\n",
    "    dnn_labels = data.groupby('customer_ID')[label_name].mean().values\n",
    "    dnn_labels = (dnn_labels > 0.5).astype(np.float32).ravel()\n",
    "    return dnn_data.values.astype(np.float32), dnn_labels\n",
    "\n",
    "train_dnn_data, train_dnn_labels = prepare_dnn_data(train_data)\n",
    "val_dnn_data, val_dnn_labels = prepare_dnn_data(val_data)\n",
    "#train_dnn_loader = DataLoader(list(zip(train_dnn_data, train_dnn_labels)), batch_size=64, shuffle=True)\n",
    "#val_dnn_loader = DataLoader(list(zip(val_dnn_data, val_dnn_labels)), batch_size=64, shuffle=True)\n",
    "test_dnn_data, _ = prepare_dnn_data(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a6757c-360a-4211-b214-5c8f3bb25cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T21:02:29.701374Z",
     "iopub.status.busy": "2023-12-25T21:02:29.701099Z",
     "iopub.status.idle": "2023-12-25T21:02:29.717351Z",
     "shell.execute_reply": "2023-12-25T21:02:29.716667Z",
     "shell.execute_reply.started": "2023-12-25T21:02:29.701354Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义新的神经网络模型\n",
    "class EnhancedNN(nn.Module):\n",
    "    def __init__(self, rnn_input_size, rnn_hidden_size, dnn_input_size, dnn_hidden_size, output_size, weight_decay=0.0):\n",
    "        super(EnhancedNN, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=rnn_input_size, hidden_size=rnn_hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(dnn_input_size, dnn_hidden_size)\n",
    "        self.fc2 = nn.Linear(rnn_hidden_size + dnn_hidden_size, output_size)\n",
    "\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, rnn_input, dnn_input):\n",
    "        if self.training:\n",
    "            _, (rnn_out, _) = self.rnn(rnn_input)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                _, (rnn_out, _) = self.rnn(rnn_input)\n",
    "        # 使用整个序列的信息，而不仅仅是最后一个时间步\n",
    "        rnn_out = rnn_out.squeeze(dim=0)\n",
    "        rnn_out = rnn_out.unsqueeze(1)  # 添加一个维度\n",
    "\n",
    "        dnn_out = self.fc1(dnn_input)\n",
    "        dnn_out = F.relu(dnn_out)\n",
    "        #print(\"rnn_out shape:\", rnn_out.shape)\n",
    "        #print(\"dnn_out shape:\", dnn_out.shape)\n",
    "\n",
    "        combined_out = torch.cat([rnn_out, dnn_out], dim=-1)\n",
    "        \n",
    "    \n",
    "        if self.weight_decay > 0.0:\n",
    "            l2_reg = torch.tensor(0.0, requires_grad=True)\n",
    "            for param in self.parameters():\n",
    "                l2_reg += torch.norm(param)\n",
    "            combined_out += self.weight_decay * l2_reg\n",
    "\n",
    "        output = self.fc2(combined_out)\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# 训练新模型\n",
    "def train_enhanced_model(model, train_loader, val_loader, criterion, optimizer, epochs=5, patience=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Training Enhanced Model...\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0  # 计数连续不改善的周期\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_idx, (rnn_input, dnn_input, labels) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}')):\n",
    "            #rnn_input, dnn_input, labels = torch.tensor(rnn_input, dtype=torch.float32), torch.tensor(dnn_input, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)\n",
    "            rnn_input, dnn_input, labels = rnn_input.to(device), dnn_input.to(device), labels.to(device)\n",
    "            #print(\"rnn_input shape:\", rnn_input.shape)\n",
    "            #print(\"dnn_input shape:\", dnn_input.shape)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rnn_input, dnn_input.unsqueeze(1))\n",
    "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Avg Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # 在验证集上评估性能\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2%}')\n",
    "\n",
    "        # 提前停止逻辑\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1} due to no improvement in validation loss.')\n",
    "                break\n",
    "\n",
    "    print('Training finished.')\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rnn_input, dnn_input, labels in tqdm(val_loader, desc='Validation'):\n",
    "            # 将NumPy数组转换为PyTorch张量\n",
    "            #rnn_input, dnn_input, labels = torch.tensor(rnn_input, dtype=torch.float32), torch.tensor(dnn_input, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)\n",
    "            \n",
    "            #print(\"rnn_input shape:\", rnn_input.shape)\n",
    "            #print(\"dnn_input shape:\", dnn_input.shape)\n",
    "            #print(\"labels shape:\", labels.shape)\n",
    "            # 将输入数据移到设备上\n",
    "            rnn_input, dnn_input, labels = rnn_input.to(device), dnn_input.to(device), labels.to(device)\n",
    "            outputs = model(rnn_input, dnn_input.unsqueeze(1))\n",
    "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "            total_val_loss += loss.item()\n",
    "            # 计算正确的预测\n",
    "            predicted_labels = (outputs > 0.5).float()\n",
    "            predicted_labels = predicted_labels.view_as(labels)\n",
    "            #correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            #total_samples += labels.numel()\n",
    "            #print(f'Outputs Shape: {outputs.shape}')\n",
    "            #print(f'Predicted Labels Shape: {predicted_labels.shape}')\n",
    "            #print(f'Labels Shape: {labels.shape}')\n",
    "            batch_correct_predictions = (predicted_labels == labels).sum().item()\n",
    "            batch_total_samples = labels.size(0)  # 使用 size(0) 以确保仅计算批次大小\n",
    "\n",
    "            correct_predictions += batch_correct_predictions\n",
    "            total_samples += batch_total_samples\n",
    "            #print(f'Batch Correct Predictions: {batch_correct_predictions}')\n",
    "            #print(f'Batch Total Samples: {batch_total_samples}')\n",
    "            #print(f'Accumulated Correct Predictions: {correct_predictions}')\n",
    "            #print(f'Accumulated Total Samples: {total_samples}')\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "    #print(f'Final Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "    return avg_val_loss, val_accuracy\n",
    "\n",
    "# 测试 Enhanced 模型\n",
    "def test_enhanced_model(model, rnn_test_data, dnn_test_data):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Testing Enhanced Model...\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"rnn_test_data shape:\", rnn_test_data.shape)\n",
    "    print(\"dnn_test_data shape:\", dnn_test_data.shape)\n",
    "\n",
    "    # 将测试数据封装进 TensorDataset\n",
    "    test_dataset = TensorDataset(torch.tensor(rnn_test_data, dtype=torch.float32), \n",
    "                                 torch.tensor(dnn_test_data, dtype=torch.float32))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    enhanced_preds = []\n",
    "    with torch.no_grad():\n",
    "        for rnn_input, dnn_input in tqdm(test_loader, desc='Enhanced Testing'):\n",
    "            rnn_input, dnn_input = rnn_input.to(device), dnn_input.to(device)\n",
    "            outputs = model(rnn_input, dnn_input.unsqueeze(1))\n",
    "            enhanced_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    print(\"Testing over.\")\n",
    "    return np.array(enhanced_preds).flatten()  # 根据需要调整输出格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8288765-b81b-416b-9a4d-d463acdb151f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T21:02:29.718132Z",
     "iopub.status.busy": "2023-12-25T21:02:29.717984Z",
     "iopub.status.idle": "2023-12-25T21:06:40.409577Z",
     "shell.execute_reply": "2023-12-25T21:06:40.408448Z",
     "shell.execute_reply.started": "2023-12-25T21:02:29.718116Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Training Enhanced Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 5721/5721 [00:12<00:00, 451.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Avg Loss: 0.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1249.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6486, Validation Accuracy: 88.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 5721/5721 [00:09<00:00, 621.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Avg Loss: 0.6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1235.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6480, Validation Accuracy: 88.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 5721/5721 [00:09<00:00, 614.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Avg Loss: 0.6472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1240.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6480, Validation Accuracy: 88.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 5721/5721 [00:09<00:00, 620.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Avg Loss: 0.6465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1272.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6470, Validation Accuracy: 88.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 5721/5721 [00:09<00:00, 630.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Avg Loss: 0.6459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1270.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6474, Validation Accuracy: 88.53%\n",
      "Training finished.\n",
      "Fold 2/5\n",
      "Training Enhanced Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 5721/5721 [00:12<00:00, 446.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Avg Loss: 0.6459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1258.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6459, Validation Accuracy: 88.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 5721/5721 [00:08<00:00, 657.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Avg Loss: 0.6453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1238.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6457, Validation Accuracy: 88.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 5721/5721 [00:07<00:00, 742.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Avg Loss: 0.6449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1264.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6456, Validation Accuracy: 88.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 5721/5721 [00:07<00:00, 746.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Avg Loss: 0.6445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1259.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6457, Validation Accuracy: 88.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 5721/5721 [00:07<00:00, 746.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Avg Loss: 0.6441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1193.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6459, Validation Accuracy: 88.85%\n",
      "Training finished.\n",
      "Fold 3/5\n",
      "Training Enhanced Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 5721/5721 [00:12<00:00, 456.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Avg Loss: 0.6445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1212.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6430, Validation Accuracy: 89.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 5721/5721 [00:09<00:00, 627.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Avg Loss: 0.6441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1249.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6436, Validation Accuracy: 89.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 5721/5721 [00:09<00:00, 625.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Avg Loss: 0.6437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1232.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6436, Validation Accuracy: 89.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 5721/5721 [00:09<00:00, 625.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Avg Loss: 0.6433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1255.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6439, Validation Accuracy: 88.91%\n",
      "Early stopping at epoch 4 due to no improvement in validation loss.\n",
      "Training finished.\n",
      "Fold 4/5\n",
      "Training Enhanced Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 5721/5721 [00:12<00:00, 447.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Avg Loss: 0.6435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1199.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6429, Validation Accuracy: 89.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 5721/5721 [00:09<00:00, 616.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Avg Loss: 0.6431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1202.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6426, Validation Accuracy: 89.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 5721/5721 [00:09<00:00, 618.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Avg Loss: 0.6427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1241.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6433, Validation Accuracy: 89.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 5721/5721 [00:09<00:00, 625.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Avg Loss: 0.6423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1250.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6437, Validation Accuracy: 89.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 5721/5721 [00:09<00:00, 620.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Avg Loss: 0.6420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1222.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6433, Validation Accuracy: 89.18%\n",
      "Early stopping at epoch 5 due to no improvement in validation loss.\n",
      "Training finished.\n",
      "Fold 5/5\n",
      "Training Enhanced Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 5721/5721 [00:12<00:00, 447.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Avg Loss: 0.6423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1245.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6418, Validation Accuracy: 89.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 5721/5721 [00:08<00:00, 693.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Avg Loss: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1230.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6420, Validation Accuracy: 89.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 5721/5721 [00:07<00:00, 738.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Avg Loss: 0.6416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1186.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6424, Validation Accuracy: 89.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 5721/5721 [00:07<00:00, 739.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Avg Loss: 0.6414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1431/1431 [00:01<00:00, 1255.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6429, Validation Accuracy: 89.08%\n",
      "Early stopping at epoch 4 due to no improvement in validation loss.\n",
      "Training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 交叉验证部分\n",
    "# 使用 StratifiedKFold 进行 K 折交叉验证\n",
    "num_folds = 5\n",
    "stratified_kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# 初始化模型、优化器和损失函数\n",
    "enhanced_model = EnhancedNN(\n",
    "    rnn_input_size=train_rnn_data.shape[2],\n",
    "    rnn_hidden_size=64,\n",
    "    dnn_input_size=train_dnn_data.shape[1],\n",
    "    dnn_hidden_size=64,\n",
    "    output_size=1\n",
    ")\n",
    "enhanced_criterion = nn.BCEWithLogitsLoss()\n",
    "enhanced_optimizer = optim.Adam(enhanced_model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练和验证模型\n",
    "for fold, (train_indices, val_indices) in enumerate(stratified_kfold.split(train_rnn_data, train_dnn_labels)):\n",
    "    print(f'Fold {fold + 1}/{num_folds}')\n",
    "\n",
    "    # 划分数据\n",
    "    fold_train_rnn_data, fold_train_dnn_data, fold_train_dnn_labels = train_rnn_data[train_indices], train_dnn_data[train_indices], train_dnn_labels[train_indices]\n",
    "    fold_val_rnn_data, fold_val_dnn_data, fold_val_dnn_labels = train_rnn_data[val_indices], train_dnn_data[val_indices], train_dnn_labels[val_indices]\n",
    "    \n",
    "    # 创建 DataLoader\n",
    "    fold_train_loader = DataLoader(list(zip(fold_train_rnn_data, fold_train_dnn_data, fold_train_dnn_labels)), batch_size=64, shuffle=True)\n",
    "    fold_val_loader = DataLoader(list(zip(fold_val_rnn_data, fold_val_dnn_data, fold_val_dnn_labels)), batch_size=64, shuffle=False)\n",
    "\n",
    "    # 训练 Enhanced 模型，传递验证集数据和提前停止的耐心参数\n",
    "    train_enhanced_model(enhanced_model, fold_train_loader, fold_val_loader, enhanced_criterion, enhanced_optimizer, epochs=5, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c0f84f-f264-4d74-a1bd-31d187ad5202",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T21:06:40.411065Z",
     "iopub.status.busy": "2023-12-25T21:06:40.410811Z",
     "iopub.status.idle": "2023-12-25T21:06:54.985094Z",
     "shell.execute_reply": "2023-12-25T21:06:54.984150Z",
     "shell.execute_reply.started": "2023-12-25T21:06:40.411038Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Enhanced Model...\n",
      "rnn_test_data shape: torch.Size([924621, 13, 2])\n",
      "dnn_test_data shape: (924621, 243)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced Testing: 100%|██████████| 14448/14448 [00:11<00:00, 1304.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing over.\n"
     ]
    }
   ],
   "source": [
    "# 在测试集上评估模型，这里需要你根据具体情况实现 test_enhanced_model 函数\n",
    "test_enhanced_preds = test_enhanced_model(enhanced_model, test_rnn_data, test_dnn_data)\n",
    "\n",
    "# 输出最终结果\n",
    "#final_result = pd.DataFrame({'customer_ID': test_data['customer_ID'].values[:len(test_enhanced_preds)], 'prob': test_enhanced_preds.squeeze()})\n",
    "final_result = pd.DataFrame({'customer_ID': test_data.drop_duplicates('customer_ID')['customer_ID'].values, 'prob': test_enhanced_preds.squeeze()})\n",
    "# 聚合测试集中相同 customer_ID 的预测结果\n",
    "#aggregated_test_preds = final_result.groupby('customer_ID')['prob'].mean().reset_index()\n",
    "# 删除重复的 customer_ID\n",
    "#final_result_unique = aggregated_test_preds.drop_duplicates('customer_ID')\n",
    "# 输出最终结果\n",
    "#final_result_aggregated = pd.DataFrame({'customer_ID': final_result_unique['customer_ID'].values, 'prob': final_result_unique['prob']})\n",
    "\n",
    "#final_result_aggregated.to_csv('./output/final_result_aggregated.csv', index=False)\n",
    "final_result.to_csv('./output/final_result.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef68a8c4-dd0b-4407-b7ae-296ec0467748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T21:06:54.986251Z",
     "iopub.status.busy": "2023-12-25T21:06:54.986062Z",
     "iopub.status.idle": "2023-12-25T21:06:55.581221Z",
     "shell.execute_reply": "2023-12-25T21:06:55.580538Z",
     "shell.execute_reply.started": "2023-12-25T21:06:54.986230Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000469ba478561f23a92a868bd366de6f6527a684c9a...</td>\n",
       "      <td>1.042290e-37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...</td>\n",
       "      <td>1.294072e-36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...</td>\n",
       "      <td>2.729212e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...</td>\n",
       "      <td>2.867651e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...</td>\n",
       "      <td>8.769653e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924616</th>\n",
       "      <td>ffff952c631f2c911b8a2a8ca56ea6e656309a83d2f64c...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924617</th>\n",
       "      <td>ffffcf5df59e5e0bba2a5ac4578a34e2b5aa64a1546cd3...</td>\n",
       "      <td>9.807990e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924618</th>\n",
       "      <td>ffffd61f098cc056dbd7d2a21380c4804bbfe60856f475...</td>\n",
       "      <td>2.852487e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924619</th>\n",
       "      <td>ffffddef1fc3643ea179c93245b68dca0f36941cd83977...</td>\n",
       "      <td>1.845468e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924620</th>\n",
       "      <td>fffffa7cf7e453e1acc6a1426475d5cb9400859f82ff61...</td>\n",
       "      <td>2.538062e-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>924621 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              customer_ID          prob\n",
       "0       00000469ba478561f23a92a868bd366de6f6527a684c9a...  1.042290e-37\n",
       "1       00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...  1.294072e-36\n",
       "2       0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...  2.729212e-23\n",
       "3       00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...  2.867651e-01\n",
       "4       00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...  8.769653e-01\n",
       "...                                                   ...           ...\n",
       "924616  ffff952c631f2c911b8a2a8ca56ea6e656309a83d2f64c...  0.000000e+00\n",
       "924617  ffffcf5df59e5e0bba2a5ac4578a34e2b5aa64a1546cd3...  9.807990e-01\n",
       "924618  ffffd61f098cc056dbd7d2a21380c4804bbfe60856f475...  2.852487e-02\n",
       "924619  ffffddef1fc3643ea179c93245b68dca0f36941cd83977...  1.845468e-09\n",
       "924620  fffffa7cf7e453e1acc6a1426475d5cb9400859f82ff61...  2.538062e-29\n",
       "\n",
       "[924621 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfdfdf = pd.read_csv('./output/final_result.csv')\n",
    "dfdfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639bf2f9-2677-4c9d-89d7-0ec185d49d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
